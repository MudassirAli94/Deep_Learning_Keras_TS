# -*- coding: utf-8 -*-
"""Soccer

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wBl1scWGOYptFMEr-bCdnHEtHU4M8AgL
"""

import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings("ignore")

from google.colab import drive
drive.mount('/content/drive')

df = pd.read_csv('drive/My Drive/Colab Notebooks/Soccer/results.csv' ,index_col=0)
df.head()

print(len(df))

df['home_team_win'] = np.nan

df['home_team_win'][df['home_score'] > df['away_score']] = "2"
df['home_team_win'][df['home_score'] < df['away_score']] = "0"
df['home_team_win'][df['home_score'] == df['away_score']] = "1"

# 0 represents home team lost, 1 represents home team tied, 2 represents home team won

df['home_team_win'] = pd.to_numeric(df['home_team_win'])
df['home_score'] = pd.to_numeric(df['home_score'])
df['away_score'] = pd.to_numeric(df['away_score'])

# We are making it numerical instead of categorical for now for a cleaner
# dataset for when we make our dummy varaibles

for i in range(0,2):
  df.iloc[:,i] = pd.Categorical(df.iloc[:,i])
for i in range(4,6):
  df.iloc[:,i] = pd.Categorical(df.iloc[:,i])

df.info()

from sklearn import preprocessing
le = preprocessing.LabelEncoder()

df.neutral = le.fit_transform(df.neutral)

df.neutral = pd.Categorical(df.neutral)

# 0 = False

df.head()

df = pd.get_dummies(df)
df.home_team_win = pd.Categorical(df.home_team_win)

df.head()

import keras
from keras import layers
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split


x_data = df.drop('home_team_win' , axis = 1)
y_data = df.home_team_win

X_train, x_test, y_train, y_test = train_test_split = train_test_split(x_data, y_data, test_size = 0.30 , random_state = 23)

#model.fit(X_train, y_train , epochs= 20, batch_size= 1000)

import keras
from keras import Sequential
from keras.layers import Dense, LSTM, GRU
from keras.layers import Flatten
from keras import Model
from keras import regularizers
from keras import optimizers
from keras.layers import Dropout
from keras.callbacks import EarlyStopping


early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')

model = Sequential()

# We are going to include a dropout layer after the input layer to avoid overfitting

model.add(Dense(64, activation='relu', input_shape=(X_train.shape[1],))), 
model.add(Dropout(0.20)),
model.add(Dense(3 , activation= 'softmax'))

learningrate = 0.001
epochs = 30
momentum= 0.7

sgd = optimizers.SGD(lr=learningrate , momentum= momentum)

model.compile(optimizer= sgd,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model.summary()

m = model.fit(X_train, y_train , epochs= epochs, batch_size= 1000 , callbacks= [early_stopping_monitor])

from keras import backend as K 

K.clear_session()

early_stopping_monitor = EarlyStopping(monitor='val_loss', patience=3, verbose=1, mode='auto')

model1 = Sequential()

# This model is to see if we can get a better accuracy if the dropout is the input layer

model1.add(Dropout(0.20 , input_shape=(X_train.shape[1],))),
model1.add(Dense(64, activation='relu',)),
model1.add(Dense(3 , activation= 'softmax'))

learningrate = 0.001
epochs = 30
momentum = 0.7

sgd = optimizers.SGD(lr=learningrate , momentum= momentum)

model1.compile(optimizer= sgd,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model1.summary()

m1 = model1.fit(X_train, y_train , epochs= epochs, batch_size= 1000 , callbacks= [early_stopping_monitor])

# It seems the best model is to use a dropout layer after the input layer

K.clear_session()

# Let's use some RNN such as LSTM and GRU and compare which model is better

model2 = Sequential()

model2.add(LSTM(64, input_shape = (X_train.shape[1],1)))
model2.add(Dropout(.2)),
model2.add(Dense(3 , activation= 'sigmoid'))


learningrate = 0.001
epochs = 3
momentum = 0.7

sgd = optimizers.SGD(lr=learningrate , momentum= momentum)

model2.compile(optimizer= sgd,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model2.summary()

m2 = model2.fit(np.array(X_train).reshape(28661,2962,1), y_train , epochs= epochs, batch_size= 500 , callbacks= [early_stopping_monitor])

K.clear_session()

model3 = Sequential()

model3.add(GRU(64, input_shape = (X_train.shape[1],1)))
model3.add(Dropout(.2)),
model3.add(Dense(3 , activation= 'sigmoid'))


learningrate = 0.001
epochs = 3
momentum = 0.7

sgd = optimizers.SGD(lr=learningrate , momentum= momentum)

model3.compile(optimizer= sgd,
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model3.summary()

m3 = model3.fit(np.array(X_train).reshape(28661,2962,1), y_train , epochs= epochs, batch_size= 500 , callbacks= [early_stopping_monitor])

K.clear_session()

import matplotlib.pyplot as plt


plt.figure(figsize= (20,7))
ax = plt.subplot(121)
pd.Series(m.history['acc'] ).plot(ax = ax)
plt.title('m-model accuracy')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

ax = plt.subplot(122)
pd.Series(m.history['loss']).plot(ax = ax)
plt.title('m-model loss')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

plt.figure(figsize= (20,7))
ax = plt.subplot(221)
pd.Series(m1.history['acc']).plot(ax = ax , c = 'red')
plt.title('m1-model accuracy; Dropout layer = Input Layer')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='bottom left')


ax = plt.subplot(222)
pd.Series(m1.history['loss']).plot(ax = ax , c = 'red')
plt.title('m1-Model loss; Dropout layer = Input Layer')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()


plt.figure(figsize= (20,7))
ax = plt.subplot(321)
pd.Series(m2.history['acc']).plot(ax = ax , c = 'g')
plt.title('m2-model accuracy LSTM')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

ax = plt.subplot(322)
pd.Series(m2.history['loss']).plot(ax = ax , c = 'g')
plt.title('m2-Model loss LSTM')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

plt.figure(figsize= (20,7))
ax = plt.subplot(321)
pd.Series(m3.history['acc']).plot(ax = ax , c = 'gold')
plt.title('m3-model accuracy GRU')
plt.ylabel('Accuracy')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper left')

ax = plt.subplot(322)
pd.Series(m3.history['loss']).plot(ax = ax , c = 'gold')
plt.title('m3-Model loss GRU')
plt.ylabel('Loss')
plt.xlabel('Epoch')
plt.legend(['Train', 'Test'], loc='upper right')
plt.show()

"""We can see that the first model, the model with the dropout layer after the input layer holds the best accuracy and loss for this particular dataset"""

import tensorflow as tf
from tensorflow.keras import layers

model4 = tf.keras.Sequential([
layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),
layers.Dropout(.2),
layers.Dense(3, activation='softmax')])

model4.compile(optimizer=tf.keras.optimizers.SGD(learning_rate= 0.001 , momentum= 0.7),
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

model4.summary()

m4 = model4.fit(X_train, y_train , epochs= 30, batch_size= 1000 , callbacks= [early_stopping_monitor])

print(min(m4.history['loss']) > min(m.history['loss']))
print(max(m4.history['acc']) > max(m.history['acc']))

# From both of our top models it seems our keras model has a lower loss function
# While our tensorflow model has a higher accuracy